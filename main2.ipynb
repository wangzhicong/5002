{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main reader\n",
    "import numpy as np \n",
    "from matplotlib import pyplot \n",
    "import pandas as pd \n",
    "\n",
    "trainset={'x':[],'y':[]}\n",
    "valset = {'x':[],'y':[]}\n",
    "\n",
    "files={}\n",
    "files['airq'] = ['aiqQuality_201804.csv','airQuality_201701-201801.csv','airQuality_201802-201803.csv']\n",
    "files['grid'] = ['gridWeather_201804.csv','gridWeather_201701-201803.csv']\n",
    "files['weather'] = ['observedWeather_201804.csv','observedWeather_201701-201801.csv','observedWeather_201802-201803.csv']\n",
    "map_air = {'stationId': 'station_id', 'utc_time': 'time', 'PM2.5': 'PM25_Concentration', 'PM10': 'PM10_Concentration', 'NO2': 'NO2_Concentration', 'CO': 'CO_Concentration', 'O3': 'O3_Concentration', 'SO2': 'SO2_Concentration'}\n",
    "map_grid = {'stationName': 'station_id', 'utc_time': 'time','wind_speed/kph':'wind_speed'}\n",
    "map_weather = { 'utc_time': 'time'}\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('aiqQuality_201804.csv')\n",
    "df2 = pd.read_csv('airQuality_201701-201801.csv').rename(columns=map_air)\n",
    "df3 = pd.read_csv('airQuality_201802-201803.csv').rename(columns=map_air)\n",
    "airq = pd.concat([df2, df3, df1], axis = 0) \n",
    "\n",
    "df1 = pd.read_csv('gridWeather_201804.csv')\n",
    "df2 = pd.read_csv('gridWeather_201701-201803.csv').rename(columns=map_grid)\n",
    "grid =  pd.concat([df2, df1], axis = 0) \n",
    "\n",
    "df1 = pd.read_csv('observedWeather_201802-201803.csv').rename(columns=map_weather)\n",
    "df2 = pd.read_csv('observedWeather_201701-201801.csv').rename(columns=map_weather)\n",
    "df3 = pd.read_csv('observedWeather_201804.csv')\n",
    "observe = pd.concat([df1,df2,df3],axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the test for 201804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 #null before 347\n",
      "After:  102\n",
      "Level 2 #null before 102\n",
      "After:  66\n",
      "Level 3 #null before 66\n",
      "After:  48\n",
      "Level 4 #null before 48\n",
      "After:  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wangzhc\\Desktop\\train\\preprocess.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[prefix+'_mean'] = rolling_obj.mean()\n",
      "C:\\Users\\Wangzhc\\Desktop\\train\\preprocess.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[prefix+'_median'] = rolling_obj.median()\n",
      "C:\\Users\\Wangzhc\\Desktop\\train\\preprocess.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[prefix+'_sum'] = rolling_obj.sum()\n",
      "C:\\Users\\Wangzhc\\Desktop\\train\\preprocess.py:125: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[prefix+'_min'] = rolling_obj.min()\n",
      "C:\\Users\\Wangzhc\\Desktop\\train\\preprocess.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[prefix+'_max'] = rolling_obj.max()\n",
      "C:\\Users\\Wangzhc\\Desktop\\train\\preprocess.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  group[prefix+'_std'] = rolling_obj.var()\n",
      "C:\\Users\\Wangzhc\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2540: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "import preprocess\n",
    "import numpy as np \n",
    "from matplotlib import pyplot \n",
    "import pandas as pd \n",
    "airq= pd.read_csv('aiqQuality_201804.csv')\n",
    "observe = pd.read_csv('observedWeather_201804.csv')\n",
    "grid= pd.read_csv('gridWeather_201804.csv')\n",
    "after_data = preprocess.preprocess(airq,observe,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wangzhc\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "after_data['weather'] = LabelEncoder().fit_transform(after_data[['weather']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset for each station\n",
    "# only take the valid set from dataset, which means it should contain both t time attribute & t+1 time label\n",
    "\n",
    "dels = ['CO_Concentration','NO2_Concentration','O3_Concentration','PM10_Concentration','PM25_Concentration','SO2_Concentration','time','id','station_id']\n",
    "def get_dataset(df,groupKey,attr,dele = True):\n",
    "    grouped = df.groupby(groupKey)\n",
    "    dataset ={}\n",
    "    for key,group in grouped:\n",
    "        attrs = []\n",
    "        labels = []\n",
    "        X = group[attr].values\n",
    "        group = group.sort_values(by='time')\n",
    "        group = group.reset_index(drop = True)\n",
    "        #print(type(group))\n",
    "        bucket = group.copy(deep=True)\n",
    "        for d in dels:\n",
    "            del bucket[d]\n",
    "        #print(bucket.columns)\n",
    "        for i in range(1,X.shape[0]):\n",
    "            if pd.to_datetime(group['time'].loc[i]) - pd.to_datetime(group['time'].loc[i-1])==pd.Timedelta(hours=1): #pd.to_datetime(time_df['END_TIME']) - pd.to_datetime(time_df['START_TIME']):\n",
    "                tmp = bucket.loc[i-1,:]\n",
    "                if pd.isna(tmp.values).any():\n",
    "                    continue\n",
    "                attrs.append(tmp.values)\n",
    "                labels.append(X[i])\n",
    "            else:\n",
    "                continue\n",
    "        attrs = np.array(attrs).reshape(-1,attrs[0].shape[0])\n",
    "        dataset[key] = (attrs,np.array(labels).reshape(-1))\n",
    "         \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset for different target and stations\n",
    "# total 3*35 (x,y) tubles\n",
    "target = ['PM25_Concentration','O3_Concentration','PM10_Concentration']\n",
    "dataset = {}\n",
    "for i in target:\n",
    "    dataset[i] = get_dataset(after_data,'station_id',i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aotizhongxin_aq\n",
      "0.2920352716792104\n",
      "0.2727559232656714\n",
      "0.24585497254946556\n",
      "badaling_aq\n",
      "0.25230856067791213\n",
      "0.3363060077938236\n",
      "0.44306737204380336\n",
      "beibuxinqu_aq\n",
      "0.23050516065293922\n",
      "0.21844576037851465\n",
      "0.3704998113098161\n",
      "daxing_aq\n",
      "0.2440218779715578\n",
      "0.448812669304619\n",
      "0.22684118997508815\n",
      "dingling_aq\n",
      "0.2593402617143255\n",
      "0.1685204874553459\n",
      "0.25876680856245093\n",
      "donggaocun_aq\n",
      "0.32413144166774854\n",
      "0.13917630512442605\n",
      "0.25295887160649905\n",
      "dongsi_aq\n",
      "0.2703766190580855\n",
      "0.23438673910610114\n",
      "0.24693243981643784\n",
      "dongsihuan_aq\n",
      "0.12021158862379694\n",
      "0.5926449865927467\n",
      "0.23683176819159932\n",
      "fangshan_aq\n",
      "0.21615417898808795\n",
      "0.26619927436558866\n",
      "0.2597828334049599\n",
      "fengtaihuayuan_aq\n",
      "0.30187382225133624\n",
      "0.26291330756475634\n",
      "0.223598741372646\n",
      "guanyuan_aq\n",
      "0.23118552658230868\n",
      "0.2007986206516563\n",
      "0.23270281253741484\n",
      "gucheng_aq\n"
     ]
    }
   ],
   "source": [
    "from lightgbm.sklearn import LGBMRegressor as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV as gdc\n",
    "import evaluation\n",
    "\n",
    "\n",
    "param = {\"max_depth\": [25,50, 75],\n",
    "        \"learning_rate\" : [0.01,0.05,0.1],\n",
    "        \"num_leaves\": [300,900,1200],\n",
    "        \"n_estimators\": [200]\n",
    "    }\n",
    "\n",
    "\n",
    "stations = list(after_data['station_id'].unique())\n",
    "for i in stations:\n",
    "    print(i)\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for j in dataset.keys():\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(scaler.fit_transform(dataset[j][i][0]), dataset[j][i][1], test_size=0.2, random_state=42)\n",
    "        search = gdc(lgbm(),param_grid= param,scoring='neg_mean_squared_error')\n",
    "        search.fit(x_train,y_train)\n",
    "        f = open('params/'+i+'_'+j+ '.txt','w')\n",
    "        f.write(str(search.best_params_))\n",
    "        f.close()\n",
    "        result = search.predict(x_test).reshape(1,-1)\n",
    "        preds.append(result)\n",
    "        labels.append(y_test.reshape(-1,1))\n",
    "        print(evaluation.smape(result,y_test))\n",
    "        \n",
    "    # one problem here is that the row num for each target is not the same, but here is just a validation\n",
    "    \n",
    "    #score = evaluation.smape(np.concatenate(whole,axis=1),np.concatenate(labels,axis=1))\n",
    "    #print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 32.0, 202.0, 191.0, 126.0, 10.0, 2.0149762339197217, 17.0,\n",
       "       2942786, 999.0532, 'aotizhongxin_aq', 27.0, '2018-04-01 09:00:00',\n",
       "       4, 197.24, 15.81, 251.16666666666666, 262.0, 1507.0, 176.0, 279.0,\n",
       "       1452.5666666666662, 234.5, 234.5, 469.0, 208.0, 261.0, 1404.5,\n",
       "       149.5, 173.5, 897.0, 45.0, 214.0, 5017.9, 251.16666666666666,\n",
       "       262.0, 1507.0, 176.0, 279.0, 1452.5666666666662, 234.5, 234.5,\n",
       "       469.0, 208.0, 261.0, 1404.5, 149.5, 173.5, 897.0, 45.0, 214.0,\n",
       "       5017.9, 251.16666666666666, 262.0, 1507.0, 176.0, 279.0,\n",
       "       1452.5666666666662, 234.5, 234.5, 469.0, 208.0, 261.0, 1404.5,\n",
       "       149.5, 173.5, 897.0, 45.0, 214.0, 5017.9, 251.16666666666666,\n",
       "       262.0, 1507.0, 176.0, 279.0, 1452.5666666666662, 234.5, 234.5,\n",
       "       469.0, 208.0, 261.0, 1404.5, 149.5, 173.5, 897.0, 45.0, 214.0,\n",
       "       5017.9, 251.16666666666666, 262.0, 1507.0, 176.0, 279.0,\n",
       "       1452.5666666666662, 234.5, 234.5, 469.0, 208.0, 261.0, 1404.5,\n",
       "       149.5, 173.5, 897.0, 45.0, 214.0, 5017.9], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((491, 97), (123, 97), (491,), (123,))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([3,4,5]).reshape(-1,1)\n",
    "b = np.array([3,4,5]).reshape(-1,1)\n",
    "c = np.array([3,4,5]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3],\n",
       "       [4, 4],\n",
       "       [5, 5]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(a,b,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3],\n",
       "       [4, 4, 4],\n",
       "       [5, 5, 5]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([a,b,c],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
