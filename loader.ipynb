{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wangzhc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate the relationship between aq_point and grid_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_grid = pd.read_csv('Beijing_grid_weather_station.csv')\n",
    "\n",
    "loc_aq = pd.read_csv('air_station.csv')\n",
    "loc_ap = loc_aq.loc[:,['name','1','2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "num = 1 # the num of nearest point to get weather information\n",
    "for i in range(len(loc_ap)):\n",
    "    name_aq = loc_aq.loc[i,['name']].values\n",
    "    dic[name_aq[0]]=[]\n",
    "    name_grid = loc_grid.loc[:,['name']].values\n",
    "    loc_aq_ = loc_aq.loc[i,['1','2']].values.reshape(1,2)\n",
    "    loc_grid_ = loc_grid.loc[:,['1','2']].values.reshape(-1,2)\n",
    "    dis = ed(X=loc_grid_,Y=loc_aq_)\n",
    "    sorted_dis = np.argsort(dis.reshape(1,-1))\n",
    "    idx = list(sorted_dis[0,0:num])\n",
    "    for j in range(num):\n",
    "        dic[name_aq[0]].append(name_grid[idx[j],0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the data, this for small dataset for one month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.read_csv('gridWeather_201804.csv')\n",
    "observe =  pd.read_csv('observedWeather_201804.csv')\n",
    "airq = pd.read_csv('aiqQuality_201804.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the data, this for small dataset for one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "airq = pd.read_csv('airQuality_201701-201801.csv')\n",
    "grid = pd.read_csv('gridWeather_201701-201803.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the column name is different , here is the name for year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_all = ['stationName','utc_time','temperature','pressure','humidity','wind_direction','wind_speed/kph']\n",
    "cat = ['temperature','pressure','humidity','wind_direction','wind_speed/kph']\n",
    "grid = grid.fillna(method=\"ffill\")\n",
    "grid = grid.loc[:,cat_all]\n",
    "from sklearn.preprocessing import scale\n",
    "for i in cat:\n",
    "    grid[i] = grid[[i]].apply(scale)\n",
    "#grid= pd.concat([grid,pd.get_dummies(grid['weather'])],axis=1)\n",
    "#del grid['weather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get attributes from weather grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?it/s]C:\\Users\\Wangzhc\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3027: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n",
      "100%|██████████| 35/35 [00:31<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data_dic={}\n",
    "for i in tqdm(dic):\n",
    "    t0 = airq[airq['stationId'] == i].sort_values(by=['utc_time'])\n",
    "    #del t0['id']\n",
    "    del t0['stationId']\n",
    "    for j in dic[i]:\n",
    "        t1 = grid[grid['stationName']== j]\n",
    "        del t1['stationName']\n",
    "        #del t1['id']\n",
    "        t1.rename((lambda x: x+'_'+j if x != 'utc_time' else x) ,inplace=True,axis ='columns')\n",
    "        t0 = t0.merge(t1,how='left',on=['utc_time'])\n",
    "    del t0['utc_time']\n",
    "    attr_num = len(t0.columns)\n",
    "    t0 = t0.fillna(method=\"ffill\")\n",
    "    t0 = t0.dropna()\n",
    "    data_dic[i] = (t0.values[0:-1,],t0.loc[:,['PM2.5', 'PM10','O3']].values[1:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from matplotlib import pyplot \n",
    "import pandas as pd \n",
    "from keras.layers.core import Dense, Activation, Dropout \n",
    "from keras.models import Sequential \n",
    "import time \n",
    "import math \n",
    "from keras.layers import LSTM \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from keras import optimizers \n",
    "import os\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers.normalization import BatchNormalization as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train one lstm model for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dongsi_aq ==============================================================================\n",
      "Train on 8028 samples, validate on 848 samples\n",
      "Epoch 1/200\n",
      " - 15s - loss: 10594.2935 - smape: 1.9286 - val_loss: 11407.7571 - val_smape: 1.9324\n",
      "Epoch 2/200\n",
      " - 10s - loss: 10338.2076 - smape: 1.8981 - val_loss: 11146.5053 - val_smape: 1.9246\n",
      "Epoch 3/200\n",
      " - 10s - loss: 10006.3078 - smape: 1.8702 - val_loss: 10897.9422 - val_smape: 1.9249\n",
      "Epoch 4/200\n",
      " - 10s - loss: 9567.8540 - smape: 1.8390 - val_loss: 10858.0351 - val_smape: 1.9332\n",
      "Epoch 5/200\n",
      " - 10s - loss: 9056.4551 - smape: 1.7889 - val_loss: 10087.4417 - val_smape: 1.8663\n",
      "Epoch 6/200\n",
      " - 10s - loss: 8498.3601 - smape: 1.7219 - val_loss: 9298.9690 - val_smape: 1.7683\n",
      "Epoch 7/200\n",
      " - 10s - loss: 7856.1055 - smape: 1.6312 - val_loss: 8512.5286 - val_smape: 1.6376\n",
      "Epoch 8/200\n",
      " - 11s - loss: 7212.0444 - smape: 1.5252 - val_loss: 7909.2814 - val_smape: 1.5865\n",
      "Epoch 9/200\n",
      " - 9s - loss: 6472.7147 - smape: 1.4120 - val_loss: 7358.7302 - val_smape: 1.4500\n",
      "Epoch 10/200\n",
      " - 10s - loss: 5785.9680 - smape: 1.3050 - val_loss: 6165.4954 - val_smape: 1.2885\n",
      "Epoch 11/200\n",
      " - 10s - loss: 5129.6456 - smape: 1.2105 - val_loss: 7063.5907 - val_smape: 1.4495\n",
      "Epoch 12/200\n",
      " - 10s - loss: 4542.6930 - smape: 1.1232 - val_loss: 5439.6458 - val_smape: 1.1913\n",
      "Epoch 13/200\n",
      " - 10s - loss: 3980.8740 - smape: 1.0438 - val_loss: 4107.5777 - val_smape: 0.9966\n",
      "Epoch 14/200\n",
      " - 10s - loss: 3484.6962 - smape: 0.9705 - val_loss: 3635.4751 - val_smape: 0.9203\n",
      "Epoch 15/200\n",
      " - 10s - loss: 3047.5950 - smape: 0.9022 - val_loss: 4777.7908 - val_smape: 1.1154\n",
      "Epoch 16/200\n",
      " - 10s - loss: 2677.7124 - smape: 0.8431 - val_loss: 2533.6063 - val_smape: 0.7520\n",
      "Epoch 17/200\n",
      " - 10s - loss: 2366.7826 - smape: 0.7963 - val_loss: 2468.4334 - val_smape: 0.7497\n",
      "Epoch 18/200\n",
      " - 9s - loss: 2069.7376 - smape: 0.7433 - val_loss: 1855.1644 - val_smape: 0.6587\n",
      "Epoch 19/200\n",
      " - 10s - loss: 1829.9152 - smape: 0.7027 - val_loss: 1593.8376 - val_smape: 0.6102\n",
      "Epoch 20/200\n",
      " - 10s - loss: 1597.0171 - smape: 0.6655 - val_loss: 1845.3073 - val_smape: 0.6350\n",
      "Epoch 21/200\n",
      " - 10s - loss: 1447.2133 - smape: 0.6316 - val_loss: 1423.3775 - val_smape: 0.5686\n",
      "Epoch 22/200\n",
      " - 10s - loss: 1270.7112 - smape: 0.6051 - val_loss: 1236.9229 - val_smape: 0.5221\n",
      "Epoch 23/200\n",
      " - 11s - loss: 1182.8816 - smape: 0.5732 - val_loss: 1205.3810 - val_smape: 0.5115\n",
      "Epoch 24/200\n",
      " - 10s - loss: 1095.5212 - smape: 0.5517 - val_loss: 1003.2474 - val_smape: 0.4674\n",
      "Epoch 25/200\n",
      " - 10s - loss: 1003.5148 - smape: 0.5265 - val_loss: 1012.3515 - val_smape: 0.4840\n",
      "Epoch 26/200\n",
      " - 10s - loss: 926.1925 - smape: 0.5150 - val_loss: 731.5872 - val_smape: 0.4250\n",
      "Epoch 27/200\n",
      " - 10s - loss: 860.7690 - smape: 0.4970 - val_loss: 1307.8387 - val_smape: 0.4940\n",
      "Epoch 28/200\n",
      " - 10s - loss: 845.9117 - smape: 0.4828 - val_loss: 724.6380 - val_smape: 0.4703\n",
      "Epoch 29/200\n",
      " - 10s - loss: 749.1306 - smape: 0.4654 - val_loss: 773.5194 - val_smape: 0.3969\n",
      "Epoch 30/200\n",
      " - 10s - loss: 729.6920 - smape: 0.4544 - val_loss: 645.6450 - val_smape: 0.4188\n",
      "Epoch 31/200\n",
      " - 10s - loss: 696.3899 - smape: 0.4446 - val_loss: 645.1058 - val_smape: 0.3966\n",
      "Epoch 32/200\n",
      " - 10s - loss: 672.2431 - smape: 0.4322 - val_loss: 596.5628 - val_smape: 0.4258\n",
      "Epoch 33/200\n",
      " - 11s - loss: 669.6470 - smape: 0.4282 - val_loss: 813.2567 - val_smape: 0.4288\n",
      "Epoch 34/200\n",
      " - 10s - loss: 615.3102 - smape: 0.4227 - val_loss: 555.6693 - val_smape: 0.3631\n",
      "Epoch 35/200\n",
      " - 10s - loss: 600.7818 - smape: 0.4162 - val_loss: 524.0730 - val_smape: 0.3528\n",
      "Epoch 36/200\n",
      " - 11s - loss: 594.1987 - smape: 0.4047 - val_loss: 532.9538 - val_smape: 0.3738\n",
      "Epoch 37/200\n",
      " - 10s - loss: 564.1937 - smape: 0.4022 - val_loss: 511.0653 - val_smape: 0.3343\n",
      "Epoch 38/200\n",
      " - 10s - loss: 573.6914 - smape: 0.4019 - val_loss: 466.2205 - val_smape: 0.3416\n",
      "Epoch 39/200\n",
      " - 10s - loss: 599.4568 - smape: 0.4003 - val_loss: 460.3646 - val_smape: 0.3416\n",
      "Epoch 40/200\n",
      " - 10s - loss: 542.1086 - smape: 0.3947 - val_loss: 431.1202 - val_smape: 0.3404\n",
      "Epoch 41/200\n",
      " - 10s - loss: 556.2626 - smape: 0.3949 - val_loss: 618.6280 - val_smape: 0.3595\n",
      "Epoch 42/200\n",
      " - 10s - loss: 538.9055 - smape: 0.3911 - val_loss: 458.0246 - val_smape: 0.3506\n",
      "Epoch 43/200\n",
      " - 9s - loss: 527.9239 - smape: 0.3916 - val_loss: 494.5089 - val_smape: 0.4004\n",
      "Epoch 44/200\n",
      " - 10s - loss: 526.8713 - smape: 0.3923 - val_loss: 464.6662 - val_smape: 0.3442\n",
      "Epoch 45/200\n",
      " - 10s - loss: 525.6545 - smape: 0.3965 - val_loss: 465.0471 - val_smape: 0.3569\n",
      "Epoch 46/200\n",
      " - 10s - loss: 529.5054 - smape: 0.3952 - val_loss: 464.5958 - val_smape: 0.3622\n",
      "Epoch 47/200\n",
      " - 10s - loss: 517.4026 - smape: 0.4029 - val_loss: 417.0797 - val_smape: 0.3539\n",
      "Epoch 48/200\n",
      " - 10s - loss: 512.1994 - smape: 0.3869 - val_loss: 659.5179 - val_smape: 0.3930\n",
      "Epoch 49/200\n",
      " - 11s - loss: 521.8052 - smape: 0.4049 - val_loss: 429.7777 - val_smape: 0.3093\n",
      "Epoch 50/200\n",
      " - 10s - loss: 480.2848 - smape: 0.3960 - val_loss: 439.7255 - val_smape: 0.3190\n",
      "Epoch 51/200\n",
      " - 9s - loss: 479.1260 - smape: 0.3967 - val_loss: 546.3207 - val_smape: 0.4016\n",
      "Epoch 52/200\n",
      " - 10s - loss: 480.7318 - smape: 0.3962 - val_loss: 514.2907 - val_smape: 0.3704\n",
      "Epoch 53/200\n",
      " - 10s - loss: 471.5803 - smape: 0.3992 - val_loss: 457.8510 - val_smape: 0.3805\n",
      "Epoch 54/200\n",
      " - 10s - loss: 465.7196 - smape: 0.4023 - val_loss: 431.1464 - val_smape: 0.3378\n",
      "Epoch 55/200\n",
      " - 10s - loss: 444.7802 - smape: 0.3936 - val_loss: 387.9808 - val_smape: 0.3544\n",
      "Epoch 56/200\n",
      " - 10s - loss: 483.2844 - smape: 0.3986 - val_loss: 414.3388 - val_smape: 0.3402\n",
      "Epoch 57/200\n",
      " - 10s - loss: 469.1523 - smape: 0.3980 - val_loss: 424.4502 - val_smape: 0.3492\n",
      "Epoch 58/200\n",
      " - 11s - loss: 479.4032 - smape: 0.4116 - val_loss: 403.8246 - val_smape: 0.3548\n",
      "Epoch 59/200\n",
      " - 11s - loss: 456.0191 - smape: 0.4043 - val_loss: 396.2264 - val_smape: 0.3581\n",
      "Epoch 60/200\n",
      " - 10s - loss: 480.9893 - smape: 0.4087 - val_loss: 560.8705 - val_smape: 0.3658\n",
      "Epoch 61/200\n",
      " - 10s - loss: 464.2770 - smape: 0.4031 - val_loss: 496.2579 - val_smape: 0.3668\n",
      "Epoch 62/200\n",
      " - 10s - loss: 450.3399 - smape: 0.4050 - val_loss: 396.6066 - val_smape: 0.3434\n",
      "Epoch 63/200\n",
      " - 11s - loss: 474.2215 - smape: 0.4097 - val_loss: 426.3256 - val_smape: 0.3388\n",
      "Epoch 64/200\n",
      " - 11s - loss: 437.7617 - smape: 0.4034 - val_loss: 496.1657 - val_smape: 0.3451\n",
      "Epoch 65/200\n",
      " - 10s - loss: 440.9386 - smape: 0.4038 - val_loss: 366.5354 - val_smape: 0.3652\n",
      "Epoch 66/200\n",
      " - 8s - loss: 458.3166 - smape: 0.4096 - val_loss: 477.1468 - val_smape: 0.3838\n",
      "Epoch 67/200\n",
      " - 8s - loss: 437.9341 - smape: 0.3957 - val_loss: 372.7066 - val_smape: 0.3513\n",
      "Epoch 68/200\n",
      " - 7s - loss: 447.7081 - smape: 0.4055 - val_loss: 734.4622 - val_smape: 0.3954\n",
      "Epoch 69/200\n",
      " - 7s - loss: 444.7602 - smape: 0.3967 - val_loss: 598.8373 - val_smape: 0.4132\n",
      "Epoch 70/200\n",
      " - 7s - loss: 428.2408 - smape: 0.4019 - val_loss: 437.8710 - val_smape: 0.3512\n",
      "Epoch 71/200\n",
      " - 7s - loss: 419.2276 - smape: 0.4007 - val_loss: 458.1844 - val_smape: 0.3261\n",
      "Epoch 72/200\n",
      " - 8s - loss: 447.9553 - smape: 0.4013 - val_loss: 437.8513 - val_smape: 0.3929\n",
      "Epoch 73/200\n",
      " - 8s - loss: 441.4411 - smape: 0.4051 - val_loss: 382.3419 - val_smape: 0.3487\n",
      "Epoch 74/200\n",
      " - 8s - loss: 431.7980 - smape: 0.3961 - val_loss: 440.1170 - val_smape: 0.3270\n",
      "Epoch 75/200\n",
      " - 8s - loss: 453.3849 - smape: 0.4163 - val_loss: 405.4418 - val_smape: 0.3364\n",
      "Epoch 76/200\n",
      " - 8s - loss: 442.0546 - smape: 0.4071 - val_loss: 403.7241 - val_smape: 0.3655\n",
      "Epoch 77/200\n",
      " - 8s - loss: 432.9165 - smape: 0.4109 - val_loss: 452.7602 - val_smape: 0.3766\n",
      "Epoch 78/200\n",
      " - 8s - loss: 429.8594 - smape: 0.4050 - val_loss: 415.9406 - val_smape: 0.3407\n",
      "Epoch 79/200\n",
      " - 8s - loss: 408.5248 - smape: 0.3965 - val_loss: 401.7721 - val_smape: 0.3856\n",
      "Epoch 80/200\n",
      " - 8s - loss: 420.1898 - smape: 0.4042 - val_loss: 598.2048 - val_smape: 0.4239\n",
      "Epoch 81/200\n",
      " - 7s - loss: 420.1571 - smape: 0.3979 - val_loss: 395.6526 - val_smape: 0.3641\n",
      "Epoch 82/200\n",
      " - 8s - loss: 426.3365 - smape: 0.4009 - val_loss: 490.5770 - val_smape: 0.3566\n",
      "Epoch 83/200\n",
      " - 8s - loss: 419.7789 - smape: 0.4057 - val_loss: 509.0940 - val_smape: 0.3532\n",
      "Epoch 84/200\n",
      " - 7s - loss: 411.8582 - smape: 0.4017 - val_loss: 371.0149 - val_smape: 0.3255\n",
      "Epoch 85/200\n",
      " - 8s - loss: 443.9328 - smape: 0.4159 - val_loss: 424.5363 - val_smape: 0.3334\n",
      "Epoch 86/200\n",
      " - 7s - loss: 429.0033 - smape: 0.4140 - val_loss: 421.1754 - val_smape: 0.3571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      " - 8s - loss: 430.1185 - smape: 0.4065 - val_loss: 510.0889 - val_smape: 0.3588\n",
      "Epoch 88/200\n",
      " - 8s - loss: 441.2211 - smape: 0.4132 - val_loss: 387.5899 - val_smape: 0.3518\n",
      "Epoch 89/200\n",
      " - 7s - loss: 423.6233 - smape: 0.4179 - val_loss: 424.0702 - val_smape: 0.3628\n",
      "Epoch 90/200\n",
      " - 7s - loss: 442.0257 - smape: 0.4222 - val_loss: 451.2076 - val_smape: 0.3378\n",
      "Epoch 91/200\n",
      " - 8s - loss: 420.3829 - smape: 0.4101 - val_loss: 422.3345 - val_smape: 0.3539\n",
      "Epoch 92/200\n",
      " - 7s - loss: 410.3175 - smape: 0.4080 - val_loss: 393.2940 - val_smape: 0.3547\n",
      "Epoch 93/200\n",
      " - 8s - loss: 413.6697 - smape: 0.4101 - val_loss: 388.8111 - val_smape: 0.3615\n",
      "Epoch 94/200\n",
      " - 8s - loss: 406.6948 - smape: 0.4113 - val_loss: 388.1123 - val_smape: 0.3353\n",
      "Epoch 95/200\n",
      " - 8s - loss: 415.8550 - smape: 0.4148 - val_loss: 611.6261 - val_smape: 0.4372\n",
      "Epoch 96/200\n",
      " - 8s - loss: 417.6471 - smape: 0.4068 - val_loss: 388.7423 - val_smape: 0.3330\n",
      "Epoch 97/200\n",
      " - 8s - loss: 411.2356 - smape: 0.4024 - val_loss: 402.1042 - val_smape: 0.3102\n",
      "Epoch 98/200\n",
      " - 8s - loss: 414.9625 - smape: 0.4115 - val_loss: 553.8031 - val_smape: 0.3956\n",
      "Epoch 99/200\n",
      " - 8s - loss: 417.7095 - smape: 0.4113 - val_loss: 383.7697 - val_smape: 0.3348\n",
      "Epoch 100/200\n",
      " - 8s - loss: 395.0050 - smape: 0.4051 - val_loss: 369.0783 - val_smape: 0.3264\n",
      "Epoch 101/200\n",
      " - 8s - loss: 400.4908 - smape: 0.4129 - val_loss: 354.5023 - val_smape: 0.3371\n",
      "Epoch 102/200\n",
      " - 7s - loss: 418.8706 - smape: 0.4079 - val_loss: 382.2500 - val_smape: 0.3460\n",
      "Epoch 103/200\n",
      " - 8s - loss: 411.2887 - smape: 0.4076 - val_loss: 404.4262 - val_smape: 0.3481\n",
      "Epoch 104/200\n",
      " - 8s - loss: 406.9109 - smape: 0.4085 - val_loss: 435.7890 - val_smape: 0.3742\n",
      "Epoch 105/200\n",
      " - 8s - loss: 401.4877 - smape: 0.4096 - val_loss: 450.2039 - val_smape: 0.3593\n",
      "Epoch 106/200\n",
      " - 8s - loss: 414.4840 - smape: 0.4008 - val_loss: 397.1596 - val_smape: 0.3387\n",
      "Epoch 107/200\n",
      " - 8s - loss: 411.6398 - smape: 0.4105 - val_loss: 479.6142 - val_smape: 0.3583\n",
      "Epoch 108/200\n",
      " - 8s - loss: 418.0746 - smape: 0.4124 - val_loss: 381.0797 - val_smape: 0.3479\n",
      "Epoch 109/200\n",
      " - 8s - loss: 425.6027 - smape: 0.4156 - val_loss: 420.5475 - val_smape: 0.3145\n",
      "Epoch 110/200\n",
      " - 8s - loss: 408.7015 - smape: 0.4086 - val_loss: 426.9425 - val_smape: 0.3452\n",
      "Epoch 111/200\n",
      " - 8s - loss: 424.3498 - smape: 0.4143 - val_loss: 368.3315 - val_smape: 0.3404\n",
      "Epoch 112/200\n",
      " - 8s - loss: 432.6126 - smape: 0.4191 - val_loss: 391.3428 - val_smape: 0.3508\n",
      "Epoch 113/200\n",
      " - 8s - loss: 406.9350 - smape: 0.4196 - val_loss: 394.2865 - val_smape: 0.3521\n",
      "Epoch 114/200\n",
      " - 8s - loss: 402.7100 - smape: 0.4101 - val_loss: 510.6734 - val_smape: 0.3845\n",
      "Epoch 115/200\n",
      " - 8s - loss: 416.3618 - smape: 0.4141 - val_loss: 406.8959 - val_smape: 0.3316\n",
      "Epoch 116/200\n",
      " - 8s - loss: 410.5098 - smape: 0.4093 - val_loss: 443.0383 - val_smape: 0.3517\n",
      "Epoch 117/200\n",
      " - 8s - loss: 412.7850 - smape: 0.4098 - val_loss: 449.1639 - val_smape: 0.3469\n",
      "Epoch 118/200\n",
      " - 8s - loss: 402.7289 - smape: 0.4101 - val_loss: 398.7235 - val_smape: 0.3807\n",
      "Epoch 119/200\n",
      " - 7s - loss: 402.5563 - smape: 0.3992 - val_loss: 384.4899 - val_smape: 0.3357\n",
      "Epoch 120/200\n",
      " - 8s - loss: 406.1315 - smape: 0.4071 - val_loss: 364.0704 - val_smape: 0.3255\n",
      "Epoch 121/200\n",
      " - 7s - loss: 409.2268 - smape: 0.4093 - val_loss: 390.8241 - val_smape: 0.3510\n",
      "Epoch 122/200\n",
      " - 7s - loss: 402.3516 - smape: 0.4069 - val_loss: 390.0883 - val_smape: 0.3704\n",
      "Epoch 123/200\n",
      " - 7s - loss: 417.0860 - smape: 0.4157 - val_loss: 404.0612 - val_smape: 0.3349\n",
      "Epoch 124/200\n",
      " - 7s - loss: 401.4488 - smape: 0.4147 - val_loss: 368.2238 - val_smape: 0.3234\n",
      "Epoch 125/200\n",
      " - 7s - loss: 418.7084 - smape: 0.4145 - val_loss: 410.3390 - val_smape: 0.3371\n",
      "Epoch 126/200\n",
      " - 7s - loss: 409.9642 - smape: 0.4101 - val_loss: 379.5019 - val_smape: 0.3501\n",
      "Epoch 127/200\n",
      " - 8s - loss: 398.7593 - smape: 0.4076 - val_loss: 372.2801 - val_smape: 0.3654\n",
      "Epoch 128/200\n",
      " - 7s - loss: 422.4138 - smape: 0.4166 - val_loss: 414.8035 - val_smape: 0.3608\n",
      "Epoch 129/200\n",
      " - 8s - loss: 390.5603 - smape: 0.4024 - val_loss: 381.5258 - val_smape: 0.3377\n",
      "Epoch 130/200\n",
      " - 7s - loss: 406.5565 - smape: 0.4238 - val_loss: 417.3145 - val_smape: 0.3634\n",
      "Epoch 131/200\n",
      " - 8s - loss: 408.9553 - smape: 0.4186 - val_loss: 562.3592 - val_smape: 0.3858\n",
      "Epoch 132/200\n",
      " - 8s - loss: 405.7265 - smape: 0.4128 - val_loss: 360.7630 - val_smape: 0.3361\n",
      "Epoch 133/200\n",
      " - 8s - loss: 397.3952 - smape: 0.4196 - val_loss: 376.7056 - val_smape: 0.3554\n",
      "Epoch 134/200\n",
      " - 8s - loss: 397.8509 - smape: 0.4171 - val_loss: 343.3409 - val_smape: 0.3593\n",
      "Epoch 135/200\n",
      " - 8s - loss: 397.5778 - smape: 0.4150 - val_loss: 428.7589 - val_smape: 0.3537\n",
      "Epoch 136/200\n",
      " - 8s - loss: 409.6440 - smape: 0.4172 - val_loss: 402.6739 - val_smape: 0.3891\n",
      "Epoch 137/200\n",
      " - 8s - loss: 400.0326 - smape: 0.4158 - val_loss: 353.7235 - val_smape: 0.3571\n",
      "Epoch 138/200\n",
      " - 8s - loss: 389.6358 - smape: 0.4151 - val_loss: 346.4628 - val_smape: 0.3400\n",
      "Epoch 139/200\n",
      " - 8s - loss: 418.0583 - smape: 0.4226 - val_loss: 668.9429 - val_smape: 0.4382\n",
      "Epoch 140/200\n",
      " - 8s - loss: 394.8155 - smape: 0.4094 - val_loss: 397.6272 - val_smape: 0.3434\n",
      "Epoch 141/200\n",
      " - 8s - loss: 396.6409 - smape: 0.4085 - val_loss: 397.1126 - val_smape: 0.3736\n",
      "Epoch 142/200\n",
      " - 8s - loss: 387.2227 - smape: 0.4079 - val_loss: 360.0716 - val_smape: 0.3346\n",
      "Epoch 143/200\n",
      " - 8s - loss: 384.8976 - smape: 0.4056 - val_loss: 398.0158 - val_smape: 0.3474\n",
      "Epoch 144/200\n",
      " - 7s - loss: 392.3738 - smape: 0.4126 - val_loss: 365.7259 - val_smape: 0.3522\n",
      "Epoch 145/200\n",
      " - 7s - loss: 391.4214 - smape: 0.4123 - val_loss: 523.4015 - val_smape: 0.3418\n",
      "Epoch 146/200\n",
      " - 8s - loss: 398.4805 - smape: 0.4147 - val_loss: 459.4001 - val_smape: 0.3508\n",
      "Epoch 147/200\n",
      " - 7s - loss: 398.6187 - smape: 0.4151 - val_loss: 384.2968 - val_smape: 0.3415\n",
      "Epoch 148/200\n",
      " - 7s - loss: 390.9952 - smape: 0.4229 - val_loss: 380.4706 - val_smape: 0.3637\n",
      "Epoch 149/200\n",
      " - 8s - loss: 396.4021 - smape: 0.4133 - val_loss: 367.1788 - val_smape: 0.3577\n",
      "Epoch 150/200\n",
      " - 8s - loss: 385.7712 - smape: 0.4076 - val_loss: 373.1850 - val_smape: 0.3463\n",
      "Epoch 151/200\n",
      " - 8s - loss: 404.5141 - smape: 0.4214 - val_loss: 359.1803 - val_smape: 0.3571\n",
      "Epoch 152/200\n",
      " - 8s - loss: 389.0224 - smape: 0.4130 - val_loss: 397.2418 - val_smape: 0.3751\n",
      "Epoch 153/200\n",
      " - 8s - loss: 408.1436 - smape: 0.4107 - val_loss: 447.1402 - val_smape: 0.3717\n",
      "Epoch 154/200\n",
      " - 8s - loss: 393.7742 - smape: 0.4157 - val_loss: 462.6660 - val_smape: 0.3594\n",
      "Epoch 155/200\n",
      " - 8s - loss: 392.3974 - smape: 0.4188 - val_loss: 460.4373 - val_smape: 0.3707\n",
      "Epoch 156/200\n",
      " - 8s - loss: 400.0246 - smape: 0.4220 - val_loss: 361.6953 - val_smape: 0.3694\n",
      "Epoch 157/200\n",
      " - 8s - loss: 387.4304 - smape: 0.4120 - val_loss: 348.2360 - val_smape: 0.3360\n",
      "Epoch 158/200\n",
      " - 8s - loss: 394.7273 - smape: 0.4150 - val_loss: 404.6799 - val_smape: 0.3640\n",
      "Epoch 159/200\n",
      " - 8s - loss: 375.0302 - smape: 0.4113 - val_loss: 355.2980 - val_smape: 0.3366\n",
      "Epoch 160/200\n",
      " - 8s - loss: 379.3107 - smape: 0.4126 - val_loss: 383.1056 - val_smape: 0.3310\n",
      "Epoch 161/200\n",
      " - 8s - loss: 406.6526 - smape: 0.4247 - val_loss: 652.2433 - val_smape: 0.4151\n",
      "Epoch 162/200\n",
      " - 8s - loss: 403.9881 - smape: 0.4174 - val_loss: 335.5225 - val_smape: 0.3614\n",
      "Epoch 163/200\n",
      " - 7s - loss: 393.6298 - smape: 0.4135 - val_loss: 365.0948 - val_smape: 0.3348\n",
      "Epoch 164/200\n",
      " - 7s - loss: 364.9946 - smape: 0.4082 - val_loss: 353.8822 - val_smape: 0.3604\n",
      "Epoch 165/200\n",
      " - 8s - loss: 398.8412 - smape: 0.4157 - val_loss: 425.8093 - val_smape: 0.3562\n",
      "Epoch 166/200\n",
      " - 7s - loss: 387.5974 - smape: 0.4216 - val_loss: 337.5368 - val_smape: 0.3551\n",
      "Epoch 167/200\n",
      " - 8s - loss: 385.3177 - smape: 0.4156 - val_loss: 395.7649 - val_smape: 0.3376\n",
      "Epoch 168/200\n",
      " - 7s - loss: 385.2710 - smape: 0.4187 - val_loss: 386.4137 - val_smape: 0.3383\n",
      "Epoch 169/200\n",
      " - 8s - loss: 373.5554 - smape: 0.4078 - val_loss: 356.4469 - val_smape: 0.3408\n",
      "Epoch 170/200\n",
      " - 7s - loss: 367.8918 - smape: 0.4088 - val_loss: 363.1996 - val_smape: 0.3657\n",
      "Epoch 171/200\n",
      " - 8s - loss: 367.5493 - smape: 0.4020 - val_loss: 369.4313 - val_smape: 0.3443\n",
      "Epoch 172/200\n",
      " - 7s - loss: 377.3290 - smape: 0.4125 - val_loss: 355.4478 - val_smape: 0.3298\n",
      "Epoch 173/200\n",
      " - 8s - loss: 386.7132 - smape: 0.4153 - val_loss: 361.8354 - val_smape: 0.3386\n",
      "Epoch 174/200\n",
      " - 7s - loss: 399.1864 - smape: 0.4245 - val_loss: 361.4632 - val_smape: 0.3896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      " - 8s - loss: 370.9567 - smape: 0.4087 - val_loss: 360.6646 - val_smape: 0.3627\n",
      "Epoch 176/200\n",
      " - 8s - loss: 379.3756 - smape: 0.4121 - val_loss: 349.7385 - val_smape: 0.3347\n",
      "Epoch 177/200\n",
      " - 8s - loss: 370.4472 - smape: 0.4074 - val_loss: 383.3011 - val_smape: 0.3482\n",
      "Epoch 178/200\n",
      " - 8s - loss: 386.3080 - smape: 0.4157 - val_loss: 345.5365 - val_smape: 0.3845\n",
      "Epoch 179/200\n",
      " - 8s - loss: 376.6167 - smape: 0.4139 - val_loss: 381.8777 - val_smape: 0.3566\n",
      "Epoch 180/200\n",
      " - 8s - loss: 376.7920 - smape: 0.4118 - val_loss: 357.0993 - val_smape: 0.3623\n",
      "Epoch 181/200\n",
      " - 7s - loss: 373.4593 - smape: 0.4148 - val_loss: 355.2357 - val_smape: 0.3513\n",
      "Epoch 182/200\n",
      " - 8s - loss: 364.1974 - smape: 0.4083 - val_loss: 364.8081 - val_smape: 0.3589\n",
      "Epoch 183/200\n",
      " - 8s - loss: 378.7551 - smape: 0.4107 - val_loss: 441.5447 - val_smape: 0.3588\n",
      "Epoch 184/200\n",
      " - 8s - loss: 363.8771 - smape: 0.4086 - val_loss: 364.8594 - val_smape: 0.3372\n",
      "Epoch 185/200\n",
      " - 8s - loss: 381.1930 - smape: 0.4181 - val_loss: 383.7563 - val_smape: 0.3587\n",
      "Epoch 186/200\n",
      " - 8s - loss: 373.3929 - smape: 0.4156 - val_loss: 352.1767 - val_smape: 0.3515\n",
      "Epoch 187/200\n",
      " - 8s - loss: 384.0174 - smape: 0.4127 - val_loss: 358.2393 - val_smape: 0.3686\n",
      "Epoch 188/200\n",
      " - 8s - loss: 379.2449 - smape: 0.4169 - val_loss: 351.9951 - val_smape: 0.3639\n",
      "Epoch 189/200\n",
      " - 8s - loss: 373.4786 - smape: 0.4197 - val_loss: 358.6940 - val_smape: 0.3505\n",
      "Epoch 190/200\n",
      " - 8s - loss: 374.7032 - smape: 0.4097 - val_loss: 761.4035 - val_smape: 0.3862\n",
      "Epoch 191/200\n",
      " - 8s - loss: 397.9933 - smape: 0.4155 - val_loss: 434.0531 - val_smape: 0.3564\n",
      "Epoch 192/200\n",
      " - 8s - loss: 363.9973 - smape: 0.4036 - val_loss: 368.0141 - val_smape: 0.3222\n",
      "Epoch 193/200\n",
      " - 8s - loss: 383.5035 - smape: 0.4177 - val_loss: 458.2020 - val_smape: 0.3941\n",
      "Epoch 194/200\n",
      " - 8s - loss: 360.5094 - smape: 0.4117 - val_loss: 519.0606 - val_smape: 0.3233\n",
      "Epoch 195/200\n",
      " - 8s - loss: 363.6787 - smape: 0.4058 - val_loss: 380.1746 - val_smape: 0.3833\n",
      "Epoch 196/200\n",
      " - 8s - loss: 376.3088 - smape: 0.4218 - val_loss: 389.8174 - val_smape: 0.3522\n",
      "Epoch 197/200\n",
      " - 8s - loss: 365.8199 - smape: 0.4148 - val_loss: 391.7284 - val_smape: 0.3662\n",
      "Epoch 198/200\n",
      " - 8s - loss: 376.5421 - smape: 0.4186 - val_loss: 451.5216 - val_smape: 0.3818\n",
      "Epoch 199/200\n",
      " - 7s - loss: 374.7665 - smape: 0.4189 - val_loss: 384.3909 - val_smape: 0.3506\n",
      "Epoch 200/200\n",
      " - 8s - loss: 376.6574 - smape: 0.4206 - val_loss: 340.2570 - val_smape: 0.3805\n",
      "tiantan_aq ==============================================================================\n",
      "Train on 8004 samples, validate on 872 samples\n",
      "Epoch 1/200\n",
      " - 12s - loss: 9355.6549 - smape: 1.9426 - val_loss: 9432.9385 - val_smape: 1.9500\n",
      "Epoch 2/200\n",
      " - 8s - loss: 9068.1190 - smape: 1.9299 - val_loss: 9274.6180 - val_smape: 1.9537\n",
      "Epoch 3/200\n",
      " - 8s - loss: 8701.1114 - smape: 1.8926 - val_loss: 9156.8823 - val_smape: 1.9465\n",
      "Epoch 4/200\n",
      " - 7s - loss: 8246.8321 - smape: 1.8338 - val_loss: 9013.3909 - val_smape: 1.9470\n",
      "Epoch 5/200\n",
      " - 8s - loss: 7737.1229 - smape: 1.7634 - val_loss: 7876.7627 - val_smape: 1.8093\n",
      "Epoch 6/200\n",
      " - 8s - loss: 7211.1738 - smape: 1.6894 - val_loss: 8038.4154 - val_smape: 1.8257\n",
      "Epoch 7/200\n",
      " - 8s - loss: 6667.6021 - smape: 1.6109 - val_loss: 6968.6712 - val_smape: 1.7089\n",
      "Epoch 8/200\n",
      " - 8s - loss: 6105.7978 - smape: 1.5268 - val_loss: 6570.0283 - val_smape: 1.5875\n",
      "Epoch 9/200\n",
      " - 8s - loss: 5571.8364 - smape: 1.4484 - val_loss: 5674.6581 - val_smape: 1.4722\n",
      "Epoch 10/200\n",
      " - 8s - loss: 5042.4510 - smape: 1.3697 - val_loss: 4829.7946 - val_smape: 1.3834\n",
      "Epoch 11/200\n",
      " - 7s - loss: 4556.6577 - smape: 1.3032 - val_loss: 5463.8642 - val_smape: 1.4747\n",
      "Epoch 12/200\n",
      " - 8s - loss: 4127.4600 - smape: 1.2478 - val_loss: 4225.5760 - val_smape: 1.3594\n",
      "Epoch 13/200\n",
      " - 8s - loss: 3749.8974 - smape: 1.1930 - val_loss: 5115.8453 - val_smape: 1.4211\n",
      "Epoch 14/200\n",
      " - 8s - loss: 3335.5889 - smape: 1.1511 - val_loss: 3087.5024 - val_smape: 1.1669\n",
      "Epoch 15/200\n",
      " - 8s - loss: 3082.8481 - smape: 1.1105 - val_loss: 2817.7750 - val_smape: 1.1227\n",
      "Epoch 16/200\n",
      " - 8s - loss: 2812.7752 - smape: 1.0824 - val_loss: 2721.3289 - val_smape: 1.0816\n",
      "Epoch 17/200\n",
      " - 7s - loss: 2622.1942 - smape: 1.0569 - val_loss: 2195.7815 - val_smape: 1.0431\n",
      "Epoch 18/200\n",
      " - 8s - loss: 2396.4375 - smape: 1.0317 - val_loss: 2446.8705 - val_smape: 1.0579\n",
      "Epoch 19/200\n",
      " - 7s - loss: 2321.8385 - smape: 1.0125 - val_loss: 2445.9030 - val_smape: 1.0461\n",
      "Epoch 20/200\n",
      " - 8s - loss: 2159.0771 - smape: 0.9906 - val_loss: 2466.4641 - val_smape: 1.0302\n",
      "Epoch 21/200\n",
      " - 8s - loss: 2050.4170 - smape: 0.9745 - val_loss: 1990.1255 - val_smape: 0.9889\n",
      "Epoch 22/200\n",
      " - 8s - loss: 1959.7025 - smape: 0.9585 - val_loss: 1539.4738 - val_smape: 0.9317\n",
      "Epoch 23/200\n",
      " - 8s - loss: 1859.3002 - smape: 0.9459 - val_loss: 1699.9725 - val_smape: 0.9246\n",
      "Epoch 24/200\n",
      " - 8s - loss: 1778.3993 - smape: 0.9296 - val_loss: 1563.5282 - val_smape: 0.9065\n",
      "Epoch 25/200\n",
      " - 7s - loss: 1726.1555 - smape: 0.9081 - val_loss: 1427.0767 - val_smape: 0.8635\n",
      "Epoch 26/200\n",
      " - 8s - loss: 1641.7489 - smape: 0.8842 - val_loss: 1482.7456 - val_smape: 0.8631\n",
      "Epoch 27/200\n",
      " - 7s - loss: 1592.3768 - smape: 0.8693 - val_loss: 1253.8413 - val_smape: 0.8358\n",
      "Epoch 28/200\n",
      " - 8s - loss: 1501.6434 - smape: 0.8490 - val_loss: 1290.1857 - val_smape: 0.8341\n",
      "Epoch 29/200\n",
      " - 8s - loss: 1409.9424 - smape: 0.8289 - val_loss: 1154.8749 - val_smape: 0.7956\n",
      "Epoch 30/200\n",
      " - 8s - loss: 1338.6379 - smape: 0.8025 - val_loss: 1071.7833 - val_smape: 0.7669\n",
      "Epoch 31/200\n",
      " - 8s - loss: 1257.0044 - smape: 0.7836 - val_loss: 1062.2785 - val_smape: 0.7585\n",
      "Epoch 32/200\n",
      " - 8s - loss: 1175.4892 - smape: 0.7570 - val_loss: 1042.7597 - val_smape: 0.7454\n",
      "Epoch 33/200\n",
      " - 8s - loss: 1129.3887 - smape: 0.7408 - val_loss: 1001.7463 - val_smape: 0.7054\n",
      "Epoch 34/200\n",
      " - 8s - loss: 1055.0959 - smape: 0.7112 - val_loss: 913.0389 - val_smape: 0.6957\n",
      "Epoch 35/200\n",
      " - 8s - loss: 954.2717 - smape: 0.6882 - val_loss: 813.7611 - val_smape: 0.6554\n",
      "Epoch 36/200\n",
      " - 8s - loss: 943.9174 - smape: 0.6695 - val_loss: 998.9429 - val_smape: 0.6497\n",
      "Epoch 37/200\n",
      " - 9s - loss: 875.5758 - smape: 0.6501 - val_loss: 681.0322 - val_smape: 0.5998\n",
      "Epoch 38/200\n",
      " - 9s - loss: 844.5121 - smape: 0.6354 - val_loss: 786.7552 - val_smape: 0.5924\n",
      "Epoch 39/200\n",
      " - 8s - loss: 811.0564 - smape: 0.6165 - val_loss: 621.8294 - val_smape: 0.5715\n",
      "Epoch 40/200\n",
      " - 8s - loss: 775.1061 - smape: 0.6089 - val_loss: 679.9326 - val_smape: 0.6034\n",
      "Epoch 41/200\n",
      " - 8s - loss: 735.8752 - smape: 0.5859 - val_loss: 542.1154 - val_smape: 0.5353\n",
      "Epoch 42/200\n",
      " - 10s - loss: 717.9249 - smape: 0.5749 - val_loss: 538.5821 - val_smape: 0.5292\n",
      "Epoch 43/200\n",
      " - 10s - loss: 661.9577 - smape: 0.5622 - val_loss: 597.3453 - val_smape: 0.5860\n",
      "Epoch 44/200\n",
      " - 11s - loss: 677.4060 - smape: 0.5627 - val_loss: 497.5543 - val_smape: 0.5005\n",
      "Epoch 45/200\n",
      " - 11s - loss: 640.6014 - smape: 0.5551 - val_loss: 486.7078 - val_smape: 0.5063\n",
      "Epoch 46/200\n",
      " - 10s - loss: 602.0665 - smape: 0.5359 - val_loss: 496.2515 - val_smape: 0.5011\n",
      "Epoch 47/200\n",
      " - 11s - loss: 613.5166 - smape: 0.5345 - val_loss: 489.1839 - val_smape: 0.5217\n",
      "Epoch 48/200\n",
      " - 11s - loss: 573.8625 - smape: 0.5204 - val_loss: 505.8934 - val_smape: 0.5236\n",
      "Epoch 49/200\n",
      " - 11s - loss: 593.4853 - smape: 0.5258 - val_loss: 545.3958 - val_smape: 0.5006\n",
      "Epoch 50/200\n",
      " - 11s - loss: 550.2133 - smape: 0.5155 - val_loss: 514.4011 - val_smape: 0.5416\n",
      "Epoch 51/200\n",
      " - 11s - loss: 568.4297 - smape: 0.5167 - val_loss: 475.7882 - val_smape: 0.5245\n",
      "Epoch 52/200\n",
      " - 11s - loss: 545.7375 - smape: 0.5138 - val_loss: 563.7871 - val_smape: 0.5511\n",
      "Epoch 53/200\n",
      " - 11s - loss: 544.3561 - smape: 0.5076 - val_loss: 463.6154 - val_smape: 0.4946\n",
      "Epoch 54/200\n",
      " - 11s - loss: 554.0991 - smape: 0.5149 - val_loss: 429.3187 - val_smape: 0.4787\n",
      "Epoch 55/200\n",
      " - 11s - loss: 563.2190 - smape: 0.5108 - val_loss: 442.7395 - val_smape: 0.5317\n",
      "Epoch 56/200\n",
      " - 11s - loss: 532.9957 - smape: 0.5099 - val_loss: 537.0873 - val_smape: 0.5503\n",
      "Epoch 57/200\n",
      " - 11s - loss: 617.5351 - smape: 0.5440 - val_loss: 516.7680 - val_smape: 0.5019\n",
      "Epoch 58/200\n",
      " - 11s - loss: 545.4117 - smape: 0.5139 - val_loss: 421.3883 - val_smape: 0.4733\n",
      "Epoch 59/200\n",
      " - 11s - loss: 567.3002 - smape: 0.5156 - val_loss: 436.9081 - val_smape: 0.4605\n",
      "Epoch 60/200\n",
      " - 11s - loss: 537.0414 - smape: 0.5052 - val_loss: 646.5576 - val_smape: 0.5437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      " - 11s - loss: 528.1195 - smape: 0.5026 - val_loss: 523.3869 - val_smape: 0.5427\n",
      "Epoch 62/200\n",
      " - 11s - loss: 525.9201 - smape: 0.5062 - val_loss: 452.8187 - val_smape: 0.4810\n",
      "Epoch 63/200\n",
      " - 11s - loss: 547.2902 - smape: 0.5055 - val_loss: 485.4682 - val_smape: 0.4668\n",
      "Epoch 64/200\n",
      " - 11s - loss: 508.9710 - smape: 0.4896 - val_loss: 418.0748 - val_smape: 0.4431\n",
      "Epoch 65/200\n",
      " - 11s - loss: 521.8408 - smape: 0.4968 - val_loss: 413.9793 - val_smape: 0.4524\n",
      "Epoch 66/200\n",
      " - 11s - loss: 517.2672 - smape: 0.4950 - val_loss: 398.6394 - val_smape: 0.4359\n",
      "Epoch 67/200\n",
      " - 11s - loss: 507.5444 - smape: 0.4976 - val_loss: 377.4497 - val_smape: 0.4349\n",
      "Epoch 68/200\n",
      " - 11s - loss: 528.7123 - smape: 0.5019 - val_loss: 501.1483 - val_smape: 0.4520\n",
      "Epoch 69/200\n",
      " - 11s - loss: 528.2105 - smape: 0.4901 - val_loss: 411.4209 - val_smape: 0.4713\n",
      "Epoch 70/200\n",
      " - 11s - loss: 515.3171 - smape: 0.4932 - val_loss: 379.3193 - val_smape: 0.4402\n",
      "Epoch 71/200\n",
      " - 11s - loss: 503.9582 - smape: 0.4904 - val_loss: 437.1239 - val_smape: 0.4396\n",
      "Epoch 72/200\n",
      " - 11s - loss: 486.2384 - smape: 0.4887 - val_loss: 552.3600 - val_smape: 0.4937\n",
      "Epoch 73/200\n",
      " - 11s - loss: 485.6122 - smape: 0.4854 - val_loss: 485.5990 - val_smape: 0.4628\n",
      "Epoch 74/200\n",
      " - 11s - loss: 502.7580 - smape: 0.4874 - val_loss: 406.6143 - val_smape: 0.4495\n",
      "Epoch 75/200\n",
      " - 11s - loss: 488.9491 - smape: 0.4920 - val_loss: 427.7493 - val_smape: 0.4510\n",
      "Epoch 76/200\n",
      " - 11s - loss: 521.1391 - smape: 0.4942 - val_loss: 465.1337 - val_smape: 0.4751\n",
      "Epoch 77/200\n",
      " - 11s - loss: 489.5700 - smape: 0.4918 - val_loss: 429.8408 - val_smape: 0.4557\n",
      "Epoch 78/200\n",
      " - 11s - loss: 479.3923 - smape: 0.4875 - val_loss: 408.3311 - val_smape: 0.4544\n",
      "Epoch 79/200\n",
      " - 11s - loss: 468.6900 - smape: 0.4882 - val_loss: 367.5333 - val_smape: 0.4478\n",
      "Epoch 80/200\n",
      " - 11s - loss: 484.0090 - smape: 0.4759 - val_loss: 711.6237 - val_smape: 0.4557\n",
      "Epoch 81/200\n",
      " - 11s - loss: 481.1546 - smape: 0.4849 - val_loss: 378.6656 - val_smape: 0.4201\n",
      "Epoch 82/200\n",
      " - 11s - loss: 463.3760 - smape: 0.4888 - val_loss: 369.6917 - val_smape: 0.4263\n",
      "Epoch 83/200\n",
      " - 11s - loss: 474.6847 - smape: 0.4936 - val_loss: 456.4433 - val_smape: 0.4863\n",
      "Epoch 84/200\n",
      " - 11s - loss: 473.3963 - smape: 0.4889 - val_loss: 377.3268 - val_smape: 0.4254\n",
      "Epoch 85/200\n",
      " - 11s - loss: 467.0708 - smape: 0.4785 - val_loss: 397.8845 - val_smape: 0.4206\n",
      "Epoch 86/200\n",
      " - 11s - loss: 442.7468 - smape: 0.4696 - val_loss: 376.4419 - val_smape: 0.4384\n",
      "Epoch 87/200\n",
      " - 11s - loss: 464.9539 - smape: 0.4786 - val_loss: 386.1822 - val_smape: 0.4299\n",
      "Epoch 88/200\n",
      " - 11s - loss: 470.7954 - smape: 0.4826 - val_loss: 433.0910 - val_smape: 0.4566\n",
      "Epoch 89/200\n",
      " - 11s - loss: 492.2627 - smape: 0.4807 - val_loss: 363.7209 - val_smape: 0.4222\n",
      "Epoch 90/200\n",
      " - 11s - loss: 456.9928 - smape: 0.4694 - val_loss: 448.2904 - val_smape: 0.4179\n",
      "Epoch 91/200\n",
      " - 11s - loss: 449.7796 - smape: 0.4674 - val_loss: 416.3704 - val_smape: 0.4382\n",
      "Epoch 92/200\n",
      " - 11s - loss: 472.4779 - smape: 0.4908 - val_loss: 460.3081 - val_smape: 0.5140\n",
      "Epoch 93/200\n",
      " - 11s - loss: 492.3603 - smape: 0.4738 - val_loss: 393.1395 - val_smape: 0.4690\n",
      "Epoch 94/200\n",
      " - 11s - loss: 459.4460 - smape: 0.4747 - val_loss: 366.3941 - val_smape: 0.4172\n",
      "Epoch 95/200\n",
      " - 11s - loss: 449.2031 - smape: 0.4703 - val_loss: 377.4290 - val_smape: 0.4213\n",
      "Epoch 96/200\n",
      " - 11s - loss: 440.6414 - smape: 0.4670 - val_loss: 398.5636 - val_smape: 0.4254\n",
      "Epoch 97/200\n",
      " - 11s - loss: 457.3655 - smape: 0.4684 - val_loss: 357.4989 - val_smape: 0.4153\n",
      "Epoch 98/200\n",
      " - 11s - loss: 465.0035 - smape: 0.4744 - val_loss: 361.8351 - val_smape: 0.4242\n",
      "Epoch 99/200\n",
      " - 11s - loss: 450.1416 - smape: 0.4676 - val_loss: 451.0705 - val_smape: 0.4367\n",
      "Epoch 100/200\n",
      " - 11s - loss: 453.6753 - smape: 0.4710 - val_loss: 442.7267 - val_smape: 0.4477\n",
      "Epoch 101/200\n",
      " - 11s - loss: 461.9212 - smape: 0.4676 - val_loss: 406.1077 - val_smape: 0.4311\n",
      "Epoch 102/200\n",
      " - 11s - loss: 450.8086 - smape: 0.4731 - val_loss: 371.6478 - val_smape: 0.4136\n",
      "Epoch 103/200\n",
      " - 11s - loss: 433.0376 - smape: 0.4622 - val_loss: 401.5515 - val_smape: 0.4364\n",
      "Epoch 104/200\n",
      " - 11s - loss: 449.0137 - smape: 0.4676 - val_loss: 459.7060 - val_smape: 0.4517\n",
      "Epoch 105/200\n",
      " - 11s - loss: 456.5513 - smape: 0.4722 - val_loss: 388.3218 - val_smape: 0.4119\n",
      "Epoch 106/200\n",
      " - 11s - loss: 428.5527 - smape: 0.4709 - val_loss: 390.5480 - val_smape: 0.4290\n",
      "Epoch 107/200\n",
      " - 11s - loss: 443.1446 - smape: 0.4610 - val_loss: 373.2083 - val_smape: 0.4161\n",
      "Epoch 108/200\n",
      " - 11s - loss: 435.6648 - smape: 0.4662 - val_loss: 401.9257 - val_smape: 0.4310\n",
      "Epoch 109/200\n",
      " - 11s - loss: 437.0088 - smape: 0.4662 - val_loss: 435.1652 - val_smape: 0.4375\n",
      "Epoch 110/200\n",
      " - 11s - loss: 441.3412 - smape: 0.4623 - val_loss: 463.8050 - val_smape: 0.4400\n",
      "Epoch 111/200\n",
      " - 11s - loss: 444.9346 - smape: 0.4676 - val_loss: 402.0292 - val_smape: 0.4046\n",
      "Epoch 112/200\n",
      " - 9s - loss: 443.3112 - smape: 0.4631 - val_loss: 439.8044 - val_smape: 0.4440\n",
      "Epoch 113/200\n",
      " - 9s - loss: 448.4280 - smape: 0.4696 - val_loss: 418.8235 - val_smape: 0.4318\n",
      "Epoch 114/200\n",
      " - 9s - loss: 431.9275 - smape: 0.4608 - val_loss: 379.0462 - val_smape: 0.4119\n",
      "Epoch 115/200\n",
      " - 9s - loss: 441.5671 - smape: 0.4668 - val_loss: 361.0385 - val_smape: 0.4105\n",
      "Epoch 116/200\n",
      " - 9s - loss: 442.4625 - smape: 0.4672 - val_loss: 377.7132 - val_smape: 0.4058\n",
      "Epoch 117/200\n",
      " - 9s - loss: 429.7646 - smape: 0.4680 - val_loss: 345.1985 - val_smape: 0.4185\n",
      "Epoch 118/200\n",
      " - 9s - loss: 420.1676 - smape: 0.4621 - val_loss: 392.0562 - val_smape: 0.4560\n",
      "Epoch 119/200\n",
      " - 9s - loss: 455.3111 - smape: 0.4708 - val_loss: 347.9682 - val_smape: 0.4438\n",
      "Epoch 120/200\n",
      " - 9s - loss: 449.7206 - smape: 0.4696 - val_loss: 395.5478 - val_smape: 0.4157\n",
      "Epoch 121/200\n",
      " - 9s - loss: 432.4304 - smape: 0.4648 - val_loss: 390.5936 - val_smape: 0.4206\n",
      "Epoch 122/200\n",
      " - 9s - loss: 435.6475 - smape: 0.4646 - val_loss: 366.0029 - val_smape: 0.4108\n",
      "Epoch 123/200\n",
      " - 9s - loss: 438.5847 - smape: 0.4646 - val_loss: 542.3238 - val_smape: 0.4492\n",
      "Epoch 124/200\n",
      " - 9s - loss: 437.7286 - smape: 0.4665 - val_loss: 364.4703 - val_smape: 0.4132\n",
      "Epoch 125/200\n",
      " - 9s - loss: 423.7767 - smape: 0.4627 - val_loss: 430.6734 - val_smape: 0.4081\n",
      "Epoch 126/200\n",
      " - 9s - loss: 435.3336 - smape: 0.4647 - val_loss: 435.0630 - val_smape: 0.4752\n",
      "Epoch 127/200\n",
      " - 9s - loss: 484.2241 - smape: 0.4830 - val_loss: 414.1967 - val_smape: 0.4281\n",
      "Epoch 128/200\n",
      " - 10s - loss: 427.7167 - smape: 0.4569 - val_loss: 387.7533 - val_smape: 0.4396\n",
      "Epoch 129/200\n",
      " - 10s - loss: 444.1213 - smape: 0.4688 - val_loss: 411.8268 - val_smape: 0.4116\n",
      "Epoch 130/200\n",
      " - 9s - loss: 440.6465 - smape: 0.4608 - val_loss: 535.0106 - val_smape: 0.4717\n",
      "Epoch 131/200\n",
      " - 9s - loss: 440.6204 - smape: 0.4684 - val_loss: 344.6568 - val_smape: 0.4117\n",
      "Epoch 132/200\n",
      " - 9s - loss: 446.3911 - smape: 0.4650 - val_loss: 374.0906 - val_smape: 0.4165\n",
      "Epoch 133/200\n",
      " - 9s - loss: 561.7365 - smape: 0.5018 - val_loss: 394.6189 - val_smape: 0.4682\n",
      "Epoch 134/200\n",
      " - 9s - loss: 476.2423 - smape: 0.4784 - val_loss: 383.8588 - val_smape: 0.4323\n",
      "Epoch 135/200\n",
      " - 9s - loss: 453.7188 - smape: 0.4761 - val_loss: 373.1831 - val_smape: 0.4422\n",
      "Epoch 136/200\n",
      " - 9s - loss: 443.7606 - smape: 0.4728 - val_loss: 398.1218 - val_smape: 0.4205\n",
      "Epoch 137/200\n",
      " - 10s - loss: 429.2961 - smape: 0.4678 - val_loss: 418.5631 - val_smape: 0.4304\n",
      "Epoch 138/200\n",
      " - 9s - loss: 405.7851 - smape: 0.4525 - val_loss: 382.5055 - val_smape: 0.4155\n",
      "Epoch 139/200\n",
      " - 9s - loss: 423.4151 - smape: 0.4677 - val_loss: 345.2843 - val_smape: 0.4249\n",
      "Epoch 140/200\n",
      " - 9s - loss: 425.9580 - smape: 0.4625 - val_loss: 358.9945 - val_smape: 0.4198\n",
      "Epoch 141/200\n",
      " - 9s - loss: 423.5904 - smape: 0.4651 - val_loss: 390.2145 - val_smape: 0.4437\n",
      "Epoch 142/200\n",
      " - 9s - loss: 416.5085 - smape: 0.4627 - val_loss: 387.1241 - val_smape: 0.4278\n",
      "Epoch 143/200\n",
      " - 9s - loss: 412.3395 - smape: 0.4656 - val_loss: 368.6512 - val_smape: 0.4094\n",
      "Epoch 144/200\n",
      " - 10s - loss: 405.4311 - smape: 0.4617 - val_loss: 387.4965 - val_smape: 0.4236\n",
      "Epoch 145/200\n",
      " - 9s - loss: 425.3031 - smape: 0.4654 - val_loss: 433.9161 - val_smape: 0.4300\n",
      "Epoch 146/200\n",
      " - 9s - loss: 419.5344 - smape: 0.4632 - val_loss: 360.2569 - val_smape: 0.4158\n",
      "Epoch 147/200\n",
      " - 9s - loss: 425.2365 - smape: 0.4706 - val_loss: 488.1241 - val_smape: 0.4413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      " - 9s - loss: 470.1146 - smape: 0.4754 - val_loss: 687.7838 - val_smape: 0.4515\n",
      "Epoch 149/200\n",
      " - 9s - loss: 428.4750 - smape: 0.4665 - val_loss: 358.7543 - val_smape: 0.4100\n",
      "Epoch 150/200\n",
      " - 9s - loss: 429.2360 - smape: 0.4656 - val_loss: 394.2280 - val_smape: 0.4345\n",
      "Epoch 151/200\n",
      " - 9s - loss: 439.5444 - smape: 0.4718 - val_loss: 340.1334 - val_smape: 0.4085\n",
      "Epoch 152/200\n",
      " - 9s - loss: 434.6316 - smape: 0.4624 - val_loss: 365.6458 - val_smape: 0.4309\n",
      "Epoch 153/200\n",
      " - 9s - loss: 425.2185 - smape: 0.4728 - val_loss: 355.9910 - val_smape: 0.4202\n",
      "Epoch 154/200\n",
      " - 9s - loss: 394.9222 - smape: 0.4588 - val_loss: 419.4678 - val_smape: 0.4464\n",
      "Epoch 155/200\n",
      " - 9s - loss: 411.9797 - smape: 0.4655 - val_loss: 372.7433 - val_smape: 0.4248\n",
      "Epoch 156/200\n",
      " - 9s - loss: 411.6300 - smape: 0.4614 - val_loss: 367.7872 - val_smape: 0.4162\n",
      "Epoch 157/200\n",
      " - 9s - loss: 425.3675 - smape: 0.4665 - val_loss: 353.1997 - val_smape: 0.4430\n",
      "Epoch 158/200\n",
      " - 9s - loss: 407.5151 - smape: 0.4573 - val_loss: 349.9293 - val_smape: 0.4032\n",
      "Epoch 159/200\n",
      " - 9s - loss: 426.7125 - smape: 0.4668 - val_loss: 408.3885 - val_smape: 0.4264\n",
      "Epoch 160/200\n",
      " - 9s - loss: 424.3598 - smape: 0.4719 - val_loss: 348.7266 - val_smape: 0.4222\n",
      "Epoch 161/200\n",
      " - 9s - loss: 417.2898 - smape: 0.4648 - val_loss: 422.8448 - val_smape: 0.5141\n",
      "Epoch 162/200\n",
      " - 9s - loss: 413.6704 - smape: 0.4678 - val_loss: 362.7181 - val_smape: 0.4191\n",
      "Epoch 163/200\n",
      " - 8s - loss: 405.8679 - smape: 0.4688 - val_loss: 357.0617 - val_smape: 0.4121\n",
      "Epoch 164/200\n",
      " - 8s - loss: 396.4942 - smape: 0.4581 - val_loss: 500.9770 - val_smape: 0.4459\n",
      "Epoch 165/200\n",
      " - 8s - loss: 418.7399 - smape: 0.4593 - val_loss: 349.7419 - val_smape: 0.4057\n",
      "Epoch 166/200\n",
      " - 8s - loss: 397.1934 - smape: 0.4541 - val_loss: 512.9524 - val_smape: 0.4122\n",
      "Epoch 167/200\n",
      " - 8s - loss: 416.1717 - smape: 0.4673 - val_loss: 359.1715 - val_smape: 0.4118\n",
      "Epoch 168/200\n"
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "for i in dic:\n",
    "    print(i,'==============================================================================')\n",
    "    attr_num = data_dic[i][0].shape[1]\n",
    "    train_X,train_y=data_dic[i]\n",
    "    train_X= train_X.reshape(-1,1,attr_num)\n",
    "    train_y = train_y.reshape(-1,3)\n",
    "    time_stamp = 10\n",
    "    trainset={'x':[],'y':[]}\n",
    "    valset={'x':[],'y':[]}\n",
    "    import random\n",
    "    for i in range(train_X.shape[0]-time_stamp+1):\n",
    "        if random.random()<0.9:\n",
    "            trainset['x'].append((train_X[i:i+time_stamp,:,:]).reshape(-1,time_stamp,attr_num))\n",
    "            trainset['y'].append((train_y[i:i+time_stamp,:]).reshape(-1,time_stamp,3))\n",
    "        else:\n",
    "            valset['x'].append((train_X[i:i+time_stamp,:,:]).reshape(-1,time_stamp,attr_num))\n",
    "            valset['y'].append((train_y[i:i+time_stamp,:]).reshape(-1,time_stamp,3))\n",
    "    trainset['x'] = np.array(trainset['x']).reshape(-1,time_stamp,attr_num)\n",
    "    valset['x'] = np.array(valset['x']).reshape(-1,time_stamp,attr_num)\n",
    "    trainset['y'] = np.array(trainset['y']).reshape(-1,time_stamp,3)\n",
    "    valset['y'] = np.array(valset['y']).reshape(-1,time_stamp,3)\n",
    "    seq = not False\n",
    "    unit = 32\n",
    "    dropout=0.0\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(unit,return_sequences=seq,dropout=dropout))\n",
    "    model.add(bn())\n",
    "    model.add(LSTM(unit//2,return_sequences=seq,dropout=dropout))\n",
    "    model.add(bn())\n",
    "    model.add(Dense(unit//4,activation='relu'))\n",
    "    model.add(bn())\n",
    "    model.add(Dense(3,activation='relu'))\n",
    "    model.compile(loss='mse', optimizer='adam',metrics=[evaluation.smape])\n",
    "    # fit network\n",
    "    #[:,-1,:].reshape(-1,3)\n",
    "    if not seq:\n",
    "        history[i] = model.fit(trainset['x'], trainset['y'][:,-1,:].reshape(-1,3),validation_data=(valset['x'],valset['y'][:,-1,:].reshape(-1,3)), epochs=1500, batch_size=64,  verbose=2, shuffle=True)\n",
    "    else:\n",
    "        history[i] = model.fit(trainset['x'], trainset['y'],validation_data=(valset['x'],valset['y']), epochs=200, batch_size=64,  verbose=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "for i in dic:\n",
    "    \n",
    "    pyplot.plot(history.history[i]['loss'], label='train')\n",
    "    pyplot.plot(history.history[i]['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(843, 10, 11)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
